{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedMadhoun/medical-rag-assistant/blob/main/Medical_RAG_Assistant_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   Medical RAG Assistant ðŸ©º\n",
        "\n",
        "\n",
        "### A Retrieval-Augmented System for Symptom-Based Disease Prediction"
      ],
      "metadata": {
        "id": "hgi_WVHkB0k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Installation"
      ],
      "metadata": {
        "id": "eaQetmxuBf0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# List of all packages we need\n",
        "packages = ['langchain', 'langgraph', 'langchain-community', 'langchain-huggingface', 'faiss-cpu', 'beautifulsoup4', 'bitsandbytes', 'pandas']\n",
        "all_installed = True\n",
        "\n",
        "def is_installed(package_name):\n",
        "    spec = importlib.util.find_spec(package_name)\n",
        "    return spec is not None\n",
        "\n",
        "# Loop through the packages and check if any are missing\n",
        "for package in packages:\n",
        "    if not is_installed(package):\n",
        "        all_installed = False\n",
        "        break\n",
        "\n",
        "if not all_installed:\n",
        "    print(\"Installing necessary packages...\")\n",
        "    !pip install langchain langgraph langchain-community langchain-huggingface faiss-cpu beautifulsoup4 bitsandbytes pandas -U -q\n",
        "else:\n",
        "    print(\"Packages already installed.\")\n",
        "\n",
        "# Define a flag file path\n",
        "restart_flag_file = \"/tmp/restart_flag\"\n",
        "\n",
        "# Check if the flag file exists (for Colab environments)\n",
        "if not os.path.exists(restart_flag_file) and not all_installed:\n",
        "    print(\"Restarting runtime to ensure correct library versions are loaded...\")\n",
        "    # Create the flag file\n",
        "    with open(restart_flag_file, \"w\") as f:\n",
        "        f.write(\"restarted\")\n",
        "    # This will restart the runtime\n",
        "    os.kill(os.getpid(), 9)\n",
        "else:\n",
        "    print(\"Runtime ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Q0KMP6BiI7",
        "outputId": "76b419bd-641a-451f-da4d-e2b8cf27747b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary packages...\n",
            "Runtime ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing and Index Building"
      ],
      "metadata": {
        "id": "PUBs1JWkHKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: Load, Chunk Convert each row of merged data into document), and Index Documents from CSVs\n",
        "\n",
        "import pandas as pd\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from typing import List\n",
        "\n",
        "print(\"Loading, processing, and preparing medical knowledge base from CSVs...\")\n",
        "\n",
        "# 1. Uploading Files\n",
        "\n",
        "df_dataset = pd.read_csv('/content/drive/MyDrive/dataset.csv')\n",
        "df_precaution = pd.read_csv('/content/drive/MyDrive/symptom_precaution.csv')\n",
        "df_description = pd.read_csv('/content/drive/MyDrive/symptom_Description.csv')\n",
        "\n",
        "# --- A. Processing the symptom file (dataset.csv) ---\n",
        "\n",
        "# Fill in all empty values â€‹â€‹in symptom columns with an empty text string ('')\n",
        "symptom_cols = [col for col in df_dataset.columns if col.startswith('Symptom_')]\n",
        "df_dataset[symptom_cols] = df_dataset[symptom_cols].fillna('')\n",
        "\n",
        "# Combine all symptoms into one column for each record, removing spaces\n",
        "def combine_symptoms(row):\n",
        "# We only collect non-empty symptoms\n",
        "    symptoms = [s.strip().replace('_', ' ') for s in row[symptom_cols] if s.strip()]\n",
        "# Removing repeated symptoms (if the symptoms recur in the same class)\n",
        "    unique_symptoms = list(set(symptoms))\n",
        "    return ', '.join(unique_symptoms)\n",
        "\n",
        "df_dataset['All_Symptoms'] = df_dataset.apply(combine_symptoms, axis=1)\n",
        "\n",
        "# Remove the original columns for symptoms (Symptom_1, Symptom_2, ...)\n",
        "df_dataset = df_dataset.drop(columns=symptom_cols)\n",
        "\n",
        "# Removing duplicate rows containing the same disease and the same combined symptoms\n",
        "df_dataset = df_dataset.drop_duplicates(subset=['Disease', 'All_Symptoms']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# --- B. Processing the precaution file (symptom_precaution.csv) ---\n",
        "\n",
        "\n",
        "# Fill in the empty values â€‹â€‹in the precautionary columns with a RAG-use string\n",
        "precaution_cols = [col for col in df_precaution.columns if col.startswith('Precaution_')]\n",
        "df_precaution[precaution_cols] = df_precaution[precaution_cols].fillna('No Further Precaution Mentioned')\n",
        "\n",
        "# Combine precautions into one column for each disease\n",
        "def combine_precautions(row):\n",
        "    precautions = [p.strip() for p in row[precaution_cols] if p.strip() != 'No Further Precaution Mentioned']\n",
        "    return ' | '.join(precautions)\n",
        "\n",
        "df_precaution['All_Precaution'] = df_precaution.apply(combine_precautions, axis=1)\n",
        "df_precaution = df_precaution.drop(columns=precaution_cols)\n",
        "df_precaution = df_precaution.drop_duplicates(subset=['Disease'])\n",
        "\n",
        "\n",
        "# --- C. Merge all files into a final DataFrame ---\n",
        "\n",
        "\n",
        "# Merge symptoms (new column All_Symptoms) with descriptions\n",
        "df_final = pd.merge(\n",
        "    df_dataset,\n",
        "    df_description,\n",
        "    on='Disease',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merge Precautions (New Column: All_Precaution)\n",
        "df_final = pd.merge(\n",
        "    df_final,\n",
        "    df_precaution,\n",
        "    on='Disease',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(\" Data merged and processed successfully. âœ…\")\n",
        "print(f\"Number of unique documents ready for indexing: {len(df_final)}\")\n",
        "print(\"\\nExample of the first merged document:\")\n",
        "print(df_final.iloc[0].to_dict())\n",
        "\n",
        "\n",
        "# --- D. Converting the final DataFrame to LangChain documents for RAG ---\n",
        "\n",
        "\n",
        "def df_to_langchain_docs(df: pd.DataFrame) -> List[Document]:\n",
        "    langchain_docs: List[Document] = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "       # Building the textual content of the document\n",
        "\n",
        "        doc_content = f\"\"\"\n",
        "        **Medical Case Knowledge**\n",
        "        Disease: {row['Disease']}\n",
        "        Symptoms: {row['All_Symptoms']}\n",
        "        Description: {row['Description']}\n",
        "        Precaution: {row['All_Precaution']}\n",
        "        \"\"\"\n",
        "\n",
        "# Create a LangChain document. Each row in df_final becomes a document.\n",
        "        langchain_docs.append(\n",
        "            Document(\n",
        "                page_content=doc_content.strip(),\n",
        "                metadata={\"source\": \"medical_dataset\", \"disease\": row['Disease']}\n",
        "            )\n",
        "        )\n",
        "    return langchain_docs\n",
        "\n",
        "final_rag_docs = df_to_langchain_docs(df_final)\n",
        "\n",
        "\n",
        "# 2. Create Embedding Model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 3. Create Vector Store (the Retriever)\n",
        "vectorstore = FAISS.from_documents(documents=final_rag_docs, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) #Bring the top 3documents\n",
        "\n",
        "print(\"âœ… Knowledge base is ready.\")\n",
        "print(f\" Â  -> Loaded {len(final_rag_docs)} unique disease entries.\")\n",
        "print(f\" Â  -> FAISS vector store created.\")"
      ],
      "metadata": {
        "id": "bxiBdQXMHNfU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649,
          "referenced_widgets": [
            "fa8d1d312a88489e9383718856c42d7d",
            "99f44b30cfaf464ea3ceefb9411200ea",
            "89907ee749984fbba4c7842ba0ffc1ea",
            "3aefca6a5e214613a391e3f637a1fe58",
            "51127658975f4e0586be4a32cf4d217c",
            "e0c6d4280c484c0a980c96cb3baf8a32",
            "c34ef72fe3de4378b8d2ddfc58401f1f",
            "b81abfae558240abaa4cf2b42b5add71",
            "11d711856f1c4c1cb5e06534e47b91f6",
            "f894ef2dfb4f41bcbb033b092d8987d4",
            "ab4659352ac841a3b705c5dcfa514c63",
            "5cf144671740435088ace38463e7f337",
            "4674d50d036b4dada9d3bb0d3ca234b6",
            "5e956ca90acd46f0b19cac3fcb4d5eea",
            "1f37fe24947642638dc5208df9e6acfe",
            "21ce68e3405242528a31ba5506fe0c2a",
            "efd921ab02514aca8361d6f6728ade90",
            "2cf18fd1ae70416780207f43febe7991",
            "9a2202a0dbb947d38bbc5a2420763cab",
            "170f91330e2f4ac4b350ecd779f489ad",
            "a0181914ff2f4853a8195aec990c661b",
            "424d551dec1b4844976e4dd20234b689",
            "4a4db8a444384e2dbc6d196ae58edcba",
            "727ba042907f4871937256c08ba5f735",
            "09ab2621a5304c968f06077d87b439b7",
            "cbb5c9c9778a445983021822b26bc766",
            "618eedfdfee946c48f9e8cc56837ed4b",
            "40a1a3d1beac47a6bac8b207ced2c88d",
            "e2349f361fae4656a1c3af2703c57700",
            "ccbb58e51c9a4574bc94927aea2f8576",
            "8cb07763d84a4df8b0eb8ef0468a96c0",
            "26152553483d455caa4fc3a5cafdd409",
            "05b83efb17a24689a5d926c9d181f9c6",
            "9c1aaddf3bcc46f8bbfc28c2d7699aea",
            "2dd67c2d3e92417f84781945986ac716",
            "e21c75e2f6ce4f08b2f15112073d892e",
            "51a5403a9cda475ab0ddcfb343725354",
            "dc056df7e6c34e5aa5c2369f53b1483f",
            "ae2175e2321540219db30f2d2faa01fb",
            "00a0fdf550b24df89dd3d4c85f2c9577",
            "f0f02103374e48b0ba65915ed951ac77",
            "f308356e19d1484282f8afef07b30e90",
            "94f4a11e82194f63be69f409484c9a57",
            "3f8cbbfd3cde4f82829ad3bc598c38ad",
            "49d1a37011344530bbc45db62040907c",
            "4045ffcaece2447d9f95e1912c8a408d",
            "ef4442dadb5149e985188d866ee77258",
            "f70fa58eb78c46908f209e6b438cb0e1",
            "8cb8360b49754ed89f5196a9091ca2b1",
            "231f95c2aefd4b10a92b9e8e59a0f3f8",
            "e05041d4eafb479fa6aba4b9c191506c",
            "6a219fa8cb584231bb7919af554a6c79",
            "d74f99f37a88408bb728414aabc9bfaf",
            "36baf27386f442c7ad72f0affc4364fb",
            "89043299dfcc462d8e73630b7e8465f5",
            "6fe21650773f431eb1873b90a70b5bcb",
            "7a0ac48b0a5544e2b7337cdf8759de8d",
            "fa5ef4dc975f48cfa4853ce0382b086a",
            "b6164c55b2fe4399b67f5db194dff6da",
            "8adaed5f56724aa9ba292ca12497d827",
            "b72b808c6d594d21929bc232dc39e576",
            "7c11bd020c9641469fb3cdbb14703b6f",
            "9b541468c8f942a9ab85aaee38b43ac7",
            "25ab5b01dcf84a3eab60df35e5ce21b6",
            "1f94f3483177429589004f7618f57bd2",
            "b2ee1fe2cee3445592f9ee0160a8cc69",
            "e8e289be30ba403b93c51bbc2180929a",
            "d0854e05ee194910acbf0893c0210fda",
            "eaf3dfd5cd3c42b69b751047cbcd02e4",
            "dfcffe3322424082aaf05e31b0df14fe",
            "83b5170c5806449b8cd4e7899d197006",
            "4d105507768b43419ccd226739b7fb30",
            "607613ccbacb46708d0d787d1f9ddfd2",
            "2e75c3d68f664834b88c63dc2b772587",
            "642605e226c4444584b3b3d770a2d096",
            "19caaa924ef14ea0a63d0e67d0c1b0b5",
            "1c2792d0c7514aa88d3499d530844cab",
            "e0c15343276e4830872403e5312e6fd5",
            "d557f777c066450ebd7967fdcf16840b",
            "7a0f00b57cc942d59fe285cd382902b2",
            "9f7a2bb33cc94c7eb845c6b0b6173dbf",
            "864d68d0b5a34c07a57f1cc6ef828395",
            "9520a026f16841a98f96ee2f65e87b7d",
            "254ffe7c69844a0e9802b58d8f635d01",
            "05943669afb14ce28bc3786a6961a066",
            "bcb760acb514420cb794bafe6bb2fb8e",
            "b53ac66f282847ad89ea312e48134c23",
            "a42418500eb84585a7224ea2abb02301",
            "0ff147381cf54d9ba06c93c55145da5f",
            "7a87a0cc223044da8828034589b18396",
            "2bf15a081c384416b978995390e21e16",
            "0f073bfe050547158b4d6fc1a218799a",
            "1117fbf97a524c19945fafe7acdff811",
            "c65c1800e0f14c6c899a3a94acb6b39a",
            "c288a9e86a054b52970ea7f4c31e0c05",
            "608e6983de3742b9af1e392e039f5024",
            "ebb78fe998544b3e90a519885a064b9c",
            "24c38a604bda4e819a097942ff59148e",
            "487c748d793a483594ba51d0fdc4b2fe",
            "8d105bbbb1bc401692d24cd06301555f",
            "336b26eac163469cb0020d8b553fd519",
            "4c597ecdecc84bbd971b13030ff6ba09",
            "6395c4b911e34ed48cbc98ce55385f65",
            "b074a3057d4c4c9f81660db0ffa914a5",
            "18252d92e5c34946bac32512c81da160",
            "88096102a0924541938668793a78c347",
            "002b7bd4463c499c9371439132436555",
            "99258f4012d047e4890c8fdacf530397",
            "5e8c7b457c3a45aba6ea7f78be2dd176",
            "9a754467f815449b91b326d8b122ca13",
            "25f5bd5b6cc842f6bec0b332bcfbe26e",
            "df5113eedb8c4a55a6168cded4a1cf77",
            "4b47d65e24544f13a4bf01e8a8db863a",
            "b4635a3a492e4d5d8f906b9f311e8915",
            "117f91d9fc68477fbb3dabad76078162",
            "5832d0f682da4532a2d1e6d171a047e0",
            "817a71125c0040ac86dcc144312cb106",
            "9737adde2d574c66b267f248a7897909",
            "06cec150f3da4febb7df469759c6d899",
            "8b0cc9e9429443deb744d701e53f73e3",
            "d46aafa6219a4ad4abfbe19ee2bc7db6"
          ]
        },
        "outputId": "d687fe70-fdcf-4556-e086-4f0315d6160f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading, processing, and preparing medical knowledge base from CSVs...\n",
            " Data merged and processed successfully. âœ…\n",
            "Number of unique documents ready for indexing: 304\n",
            "\n",
            "Example of the first merged document:\n",
            "{'Disease': 'Fungal infection', 'All_Symptoms': 'dischromic  patches, nodal skin eruptions, skin rash, itching', 'Description': 'In humans, fungal infections occur when an invading fungus takes over an area of the body and is too much for the immune system to handle. Fungi can live in the air, soil, water, and plants. There are also some fungi that live naturally in the human body. Like many microbes, there are helpful fungi and harmful fungi.', 'All_Precaution': 'bath twice | use detol or neem in bathing water | keep infected area dry | use clean cloths'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa8d1d312a88489e9383718856c42d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cf144671740435088ace38463e7f337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a4db8a444384e2dbc6d196ae58edcba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c1aaddf3bcc46f8bbfc28c2d7699aea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49d1a37011344530bbc45db62040907c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fe21650773f431eb1873b90a70b5bcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8e289be30ba403b93c51bbc2180929a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0c15343276e4830872403e5312e6fd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ff147381cf54d9ba06c93c55145da5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d105bbbb1bc401692d24cd06301555f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25f5bd5b6cc842f6bec0b332bcfbe26e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Knowledge base is ready.\n",
            " Â  -> Loaded 304 unique disease entries.\n",
            " Â  -> FAISS vector store created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Generator LLM and Graph State"
      ],
      "metadata": {
        "id": "jOMDfigyMArG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Define Generator LLM and Graph State\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# 1. Load the Generator LLM (DeepSeek)\n",
        "# We'll load it in 4-bit precision to save memory in Colab!\n",
        "# model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# Configuration for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer and model with quantization\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config, # Use the BitsAndBytesConfig\n",
        "    device_map=\"auto\" # Automatically use the GPU if available\n",
        ")\n",
        "\n",
        "# Create a Hugging Face Pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512 # Set a limit on the answer length\n",
        ")\n",
        "\n",
        "# Wrap it for LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(f\"âœ… Generator LLM ({model_id}) loaded.\")\n",
        "\n",
        "# 2. Define the Graph State\n",
        "# This is the 'memory' of our agent as it runs.\n",
        "# It holds the question, the documents we find, and the final answer.\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: The user's question\n",
        "        documents: The list of retrieved documents\n",
        "        generation: The LLM's final answer\n",
        "        question_history: A list of all questions asked, to prevent loops\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    generation: str\n",
        "    question_history: List[str] # Added for the exercise to prevent loops\n",
        "\n",
        "print(\"âœ… GraphState defined.\")"
      ],
      "metadata": {
        "id": "2Rb0KylbMBq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453,
          "referenced_widgets": [
            "8c93fab087344a16a19b4a79c55c9b6f",
            "1f8c06489a494e9eaaabe1a40347c5d8",
            "15c1dff3b8864b6ca560193bc5ff2085",
            "017fe1425e9b49a9ac69ad91469fc626",
            "dfd096e576de41a5b034a3caf2dbcb0b",
            "2b46f10a22de4cb6af88112ddf393b75",
            "8d31a16c5e8d47fd8f41d63c9c8c1d5c",
            "9fc839c57aef48408c9d67afa61db5a2",
            "96fbd7fce40e4664a695219028cd8dc3",
            "b9b381ce1de54cd78d11533f6cd7f62a",
            "fea9439058c740389d1d2f8662d58cc8",
            "1356970d592b4ff5bba3e17491c9b5e6",
            "0f498abd001f4f28afe9ce80b4dc37ef",
            "c624e1ec4e0a4644914095806d20160c",
            "d5d56a6c564443c2a94482c2242d6a3b",
            "50891ca67d864f65abd1ac452a316b36",
            "edf413e349ec4bbeb9c1518ef7bcb7e7",
            "d99c36e98327482c836eeadc427f529a",
            "1f4b803f743a49e3b1443ae7188b343b",
            "d3e47eee65b04a09b3e00ada6864a7b8",
            "955e733f4f6b4893b9edd90ab9b07456",
            "7b0574e5a6a64af1b0b0439853a2e4a6",
            "5092aee6b48c4cf18b94b3e1827422d8",
            "6c1f26b37da94f218cbd0deae52b730c",
            "53d1fa4dc4b640558ce523fde8411ec7",
            "120558aee282445e8a0b1932e969197c",
            "059ed5ad84ca4940aeb7e3bc9aec7c3d",
            "71cd946c907b43469097ed7d7eeafd46",
            "174c2db11eba429f8640a5a92ad0b97c",
            "e1fa07fcf2034ff193856b83e172fa29",
            "668384cc476a4796aec62b08aa7471a7",
            "a0c2449ffb7a4f5dae781b7521b6dc2e",
            "e305871f9ea049d79191e1442db368c5",
            "bc5264b10de74e9199d7e108cbef50e8",
            "d808fdf711df408786c5bed5189b2111",
            "ad70cfde9d1a46eebd0a9781320956a4",
            "729ef87016724b7f804c7e6d06678ade",
            "df94a64cd8b24acdb58c28ae513dc428",
            "680489ae9460470d9644c083f7543a02",
            "863d2c14fb3140d9804c4ee6d2e7c486",
            "ee6b5792aa3248eb83b3d726e8249a5d",
            "e12b1bcc41f042fcb92920308f735b78",
            "e38539de268b463cadb2cc0875516d84",
            "c60abb8dec24497180db0bc0b12f23e0",
            "ed43b0710dfa4c27944aaf90b945b588",
            "d9eb985b26d2471db9cb524fc4478909",
            "bdc7498d256e4553ab60bece32f4b806",
            "bdd9a37a656046849f4ce547bdad6ae8",
            "3a10888f31d640e3824c95f44e2e3487",
            "f5f91a5fc9554a5587f96c6c28f5ea6b",
            "647cfe3b1e7c4133ae115a15ffbc6077",
            "c1bf6e64cb74426a9c9ff66bf4561a00",
            "8c73ea88bbd64cc9aa1311036891461b",
            "9ba597ae9c8b44cda42a2eb9b7c7f7c5",
            "2d8d33d95d894fdbaaa5da5778813fef",
            "4aed4dbf404c48b092d85b06cc1da028",
            "199a5c5c08c148e9902b508879447678",
            "216148b9378747bd8aa4e54d13577847",
            "7f71e1d1f7a84c568f7f48ed5ef70a51",
            "15ad80c92e7e4355b42f9865eb07812d",
            "4db74fbc26e9465dbfc3230d3355b219",
            "46d812c18d8d490ab9f2f915ed96eb14",
            "eb990122baf744d98bedc69d7e9a3319",
            "7607935514c44429a1015604efda2623",
            "4d52825233834d0182126eb7c0eca492",
            "3f0c93d256b1481e898d77ba4e3f7bf0",
            "48132006bf0549e0b5aa49506e020a55",
            "923d1b57b70949739e14f76a7855e5cd",
            "0a6ff08a2676490586320296b0fcf79c",
            "8b5a0fa933334c2e93191420fb505b8e",
            "4eb769141bb34ec8b5d2ae6581acf1c8",
            "7154629fed964dfcbb2701cea4d8b01a",
            "70ffa907d2f7415a8c13c9edd0ec7b9a",
            "a2539a661d904d58ab584cf652bf37b7",
            "4a48a3469295497fb2f66fdae8a3dd70",
            "0823355394584ff09821b6bc9cfec37b",
            "967b7d8924344e9fb6813d1fdef2a32a",
            "111caef05b844a5ea25e29ecb0028ae4",
            "7a96243d38f140b7811c50faf3efda13",
            "01e063443f3047029dc2c962adb7de69",
            "0f4e8aecd9f64f5c9a809f381302093b",
            "3d5433d520704bab99d8d6dbccf0693c",
            "b0575d4ef1654f88b0ececa0f93fd16b",
            "0a483e4c4333406f9aad93921705de3e",
            "45a7223e5b6d4f37946132943948de53",
            "458d3c19fb0d46688a4c28ec84ee8826",
            "9f0ed194505b4041b70cb92f857a93cd",
            "2c783e3d2b2a4de7938da4ee39969c4f",
            "76b3458dd06842cea57851ec1aed703f",
            "a7acf31e99194dc0b94a9553c22c35bd",
            "7a3b5ffbac534a3c808d45daf971077d",
            "d09af577c4844037be40e49559b0b68a",
            "c2a160ec0cf9465d8faee555fc7c7610",
            "b18d0707f8084564806909f6cb3c7db0",
            "bd2cc1afe6de44ab9e661dbbe6e0e175",
            "171e7e5824b945c8b8c185f9e664859e",
            "e0b5125958be4761941a9eab35dedd4c",
            "86ae824d11c142b79f67bd3e8adad23e",
            "c9372111dd1d481996b6969eedb03177",
            "af805695569241edac5f9c4744fa59a3",
            "dd0e07a8afba4b619b943e37d3c85f23",
            "259fcf36ac814ae6b10fb6a66c54cacc",
            "1209ea463fc54e50beeec841a78cd0f8",
            "c5db866d44174772b8ddaad9df438c1d",
            "fb4d09016dd64bf8859a36761605d1ff",
            "615127f261294c1dba74dba8b76f39c3",
            "22c51da383cc4d6b93109bd15d9b1a04",
            "7024ab49d4e442f4b3a2ab5bb38c08e3",
            "9cfb97bb8c1c463ca76c4e76a7b6995e",
            "0c1f30ca9e714328a2c188fd83a9c7f9",
            "997971d151fe40678a549e56fe87cb6e",
            "8ba0efb964e34f418032aed0251e24f4",
            "869ea76ecec84cfcb1249bc9e54e747a",
            "d302bb155c9a4edda429864dbcd55ea6",
            "f70d544728f24a778c7f1baa89c4c11f",
            "ad2849388e624feda2fd316b47cb98f8",
            "b9f607863a99450896712d733f4171af",
            "f90d828a7ff6472d9378dbb1964fdfb6",
            "9af5a0d3e0624508a66c46b10136aac1",
            "80be48e8465b4259a62932cd25630a2a",
            "a4da721c67c34074b954396c2ddb1f8c",
            "48e54e038dc24a50b203d4988f158ec1",
            "b85bb952ce964925ac81243f1cdbbb76",
            "acbc206c9e31415c928295986d15044f",
            "2745afc30f804639a110ca8c0bd686ba",
            "287558dc3b0244d699abbd58547ddee2",
            "400628fcc1f54a70af5b436c9bed7d92",
            "8155b483d29547a5aeacd50e3ecb7b6f",
            "55769abeb6e9485b9fdc2c6f969deeda",
            "149fa4f0b0eb44d7a3660bfb2547e489",
            "f740cba434fd4178aaeadbdee9ce2d4c",
            "89dee6b5cfdb433e8290271fdd708a88"
          ]
        },
        "outputId": "25115391-24d7-410e-f78f-c45386b7381c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c93fab087344a16a19b4a79c55c9b6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1356970d592b4ff5bba3e17491c9b5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5092aee6b48c4cf18b94b3e1827422d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc5264b10de74e9199d7e108cbef50e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed43b0710dfa4c27944aaf90b945b588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4aed4dbf404c48b092d85b06cc1da028"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48132006bf0549e0b5aa49506e020a55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "111caef05b844a5ea25e29ecb0028ae4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b3458dd06842cea57851ec1aed703f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af805695569241edac5f9c4744fa59a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "997971d151fe40678a549e56fe87cb6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48e54e038dc24a50b203d4988f158ec1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Generator LLM (microsoft/Phi-3-mini-4k-instruct) loaded.\n",
            "âœ… GraphState defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Graph Nodes"
      ],
      "metadata": {
        "id": "NDEC3njdNn6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "print(\"Defining graph nodes...\")\n",
        "\n",
        "# --- NODE 1: RETRIEVE ---\n",
        "def retrieve_node(state: TypedDict) -> TypedDict:\n",
        "    \"\"\"Takes the question and retrieves documents (Medical Cases).\"\"\"\n",
        "    print(\"---NODE: RETRIEVE \")\n",
        "    question = state[\"question\"]\n",
        "    # Retrieve top documents using the retriever defined in Code Cell 2\n",
        "    documents = retriever.invoke(question)\n",
        "    doc_texts = [doc.page_content for doc in documents]\n",
        "\n",
        "    return {\n",
        "        \"documents\": doc_texts,\n",
        "        \"question\": question,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 2: GRADE DOCUMENTS ---\n",
        "def grade_documents_node(state: TypedDict) -> TypedDict:\n",
        "    \"\"\"Checks if the retrieved documents are relevant to the medical question.\"\"\"\n",
        "    print(\"---NODE: GRADE DOCUMENTS \")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "#A valid claim should contain only 'document' and 'question'\n",
        "    prompt_template = \"\"\"You are a medical grader. Your job is to check if a\n",
        "    retrieved medical case (containing a disease and symptoms) is relevant to the user's question about symptoms.\n",
        "    Respond with a *single word*: 'yes' if relevant, 'no' if not.\n",
        "\n",
        "    Document (Medical Case): {document}\n",
        "    User Question: {question}\n",
        "\n",
        "    Answer (yes/no):\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "# Here only the required variables are passed: 'question' and 'document'\n",
        "    grader_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    relevant_docs = []\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            result = grader_chain.invoke({\"question\": question, \"document\": doc})\n",
        "            score = result.strip().lower()\n",
        "            if \"yes\" in score:\n",
        "                print(\"  -> Grader Decision: Relevant \")\n",
        "                relevant_docs.append(doc)\n",
        "            else:\n",
        "                print(\"  -> Grader Decision: NOT Relevant\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Grader: Error - {e}\")\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"documents\": relevant_docs,\n",
        "        \"question\": question,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 3: GENERATE ---\n",
        "# --- NODE 3: GENERATE ---\n",
        "def generate_node(state: TypedDict) -> TypedDict:\n",
        "    \"\"\"Takes the question and medical documents, and generates a diagnosis and precautions.\"\"\"\n",
        "    print(\"---NODE: GENERATE ---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG prompt adapted for providing diagnosis and precautions\n",
        "    prompt_template = \"\"\"\n",
        "You are a highly disciplined medical diagnosis assistant.\n",
        "Your task is to analyze the 'Medical Context' and the user's 'Symptoms' to provide a diagnosis report.\n",
        "\n",
        "**STRICT INSTRUCTIONS (MUST FOLLOW):**\n",
        "1. Output your answer ONLY as a JSON object enclosed in triple backticks (```json ... ```).\n",
        "2. DO NOT include any explanatory text, preambles, or disclaimers outside of the JSON block.\n",
        "3. The JSON object MUST have the EXACT keys: \"Disease\", \"Key_Symptoms\" (as a list of strings), \"Description\" (as a string), \"Precautions\" (as a list of strings), and \"Confidence_Level\" (as a string).\n",
        "4. Use ONLY the data provided in the 'Medical Context'. DO NOT hallucinate.\n",
        "5. If no clear diagnosis is possible, set \"Disease\" to \"Undetermined\".\n",
        "\n",
        "**CONFIDENCE LEVEL RULES (MUST FOLLOW):**\n",
        "Determine the \"Confidence_Level\" based ONLY on how many symptoms from the user's input\n",
        "match the symptoms associated with the disease in the Medical Context:\n",
        "\n",
        "- If 3 or more symptoms match â†’ \"High\"\n",
        "- If exactly 2 symptoms match â†’ \"Medium\"\n",
        "- If exactly 1 symptom matches â†’ \"Low\"\n",
        "- If 0 symptoms match â†’ \"Very Low\"\n",
        "\n",
        "Symptoms: {question}\n",
        "Medical Context: {context}\n",
        "\n",
        "JSON Report:\n",
        "\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "    # llm is assumed to be defined globally\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    raw_generation = rag_chain.invoke({\"context\": \"\\n\".join(documents), \"question\": question})\n",
        "\n",
        "    import re\n",
        "    import json\n",
        "\n",
        "    match = re.search(r'```json\\s*(\\{.*?\\}\\s*)```', raw_generation, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        json_string = match.group(1).strip()\n",
        "    else:\n",
        "        match = re.search(r'(\\{.*?\\}\\s*)', raw_generation, re.DOTALL)\n",
        "        if match:\n",
        "            json_string = match.group(1).strip()\n",
        "        else:\n",
        "            final_generation = raw_generation\n",
        "            print(\" Â -> Error: Could not find valid JSON. Returning raw output.\")\n",
        "            return {\n",
        "                \"documents\": documents,\n",
        "                \"question\": question,\n",
        "                \"generation\": final_generation,\n",
        "                \"question_history\": state.get(\"question_history\", [])\n",
        "            }\n",
        "\n",
        "    try:\n",
        "        report = json.loads(json_string)\n",
        "        final_generation = \"```json\\n\" + json.dumps(report, indent=4) + \"\\n```\"\n",
        "    except Exception as e:\n",
        "        final_generation = raw_generation\n",
        "        print(f\" Â -> Error: JSON parsing failed after extraction: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"documents\": documents,\n",
        "        \"question\": question,\n",
        "        \"generation\": final_generation,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 4: REWRITE ---\n",
        "def rewrite_node(state: TypedDict) -> TypedDict:\n",
        "    \"\"\"Rewrites the question for a better search query, with loop prevention.\"\"\"\n",
        "    print(\"---NODE: REWRITE QUERY ---\")\n",
        "    question = state[\"question\"]\n",
        "    question_history = state.get(\"question_history\", [])\n",
        "\n",
        "    # Loop Prevention check\n",
        "    if question in question_history:\n",
        "        print(\"  -> Detected loop: Ending.\")\n",
        "        return {\n",
        "            \"documents\": [],\n",
        "            \"generation\": \"Could not find relevant information after multiple attempts.\",\n",
        "            \"question_history\": question_history\n",
        "        }\n",
        "\n",
        "    question_history.append(question)\n",
        "\n",
        "    # Rewrite prompt adapted for symptom queries\n",
        "    prompt_template = \"\"\"You are a query rewriter. Rewrite the following question (which is a list of symptoms) to be\n",
        "    a concise and specific search query for a medical vector database.\n",
        "    Respond ONLY with the rewritten query, nothing else.\n",
        "\n",
        "    Original Question: {question}\n",
        "\n",
        "    Rewritten Query:\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "    rewrite_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    new_question = rewrite_chain.invoke({\"question\": question}).strip()\n",
        "    print(f\"  -> Rewritten question: {new_question}\")\n",
        "\n",
        "    return {\n",
        "        \"question\": new_question,\n",
        "        \"question_history\": question_history,\n",
        "        \"documents\": []\n",
        "    }\n",
        "\n",
        "print(\"âœ… All nodes defined.\")"
      ],
      "metadata": {
        "id": "XnSrw0LrNpPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c51bc5b-4bcd-412a-85a7-94d6d11a5001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining graph nodes...\n",
            "âœ… All nodes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Graph Edges and Compile"
      ],
      "metadata": {
        "id": "yFIh3tu0NzMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 5: Define Graph Edges and Compile\n",
        "from langgraph.graph import END, StateGraph\n",
        "from typing import List, Dict\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "print(\"Defining conditional logic...\")\n",
        "\n",
        "def decide_to_generate_or_rewrite(state: TypedDict) -> str:\n",
        "    \"\"\"Decides whether to generate an answer, rewrite the query, or end.\"\"\"\n",
        "    print(\"---CONDITIONAL EDGE \")\n",
        "    documents = state[\"documents\"]\n",
        "    question_history = state.get(\"question_history\", [])\n",
        "\n",
        "    # Check for loop prevention (End if generation already occurred)\n",
        "    if state.get(\"generation\"):\n",
        "        print(\"  -> Decision: Loop detected or generation already done. END.\")\n",
        "        return \"end\"\n",
        "\n",
        "    # 1. If we have relevant documents, generate\n",
        "    if len(documents) > 0:\n",
        "        print(\"  -> Decision: Relevant documents found. GENERATE.\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        # 2. If no documents, check rewrite limit\n",
        "        if len(question_history) >= 2: # 1 original + 2 rewrites = 3 attempts total\n",
        "            print(\"  -> Decision: Max rewrites reached. END.\")\n",
        "            return \"end\" # Give up\n",
        "        else:\n",
        "            # 3. If no documents and limit not reached, rewrite\n",
        "            print(\"  -> Decision: No relevant documents. REWRITE.\")\n",
        "            return \"rewrite\"\n",
        "\n",
        "print(\"âœ… Conditional logic defined.\")"
      ],
      "metadata": {
        "id": "qZRKjnKVN0Gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853d8e0a-1579-4813-9f27-6e3cfe8489be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining conditional logic...\n",
            "âœ… Conditional logic defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 6: Build and Compile the Graph\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "print(\"Building the graph...\")\n",
        "\n",
        "# 1. Create the StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# 2. Add all 4 nodes\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"grade_documents\", grade_documents_node)\n",
        "workflow.add_node(\"generate\", generate_node)\n",
        "workflow.add_node(\"rewrite\", rewrite_node)\n",
        "\n",
        "# 3. Set the entry point\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# 4. Add the standard edges\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_edge(\"rewrite\", \"retrieve\") # The loop\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# 5. Add the conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate_or_rewrite,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"rewrite\": \"rewrite\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# 6. Compile the graph\n",
        "app = workflow.compile()\n",
        "print(\"âœ… LangGraph compiled and ready to run.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9Nq9Kja_nYb",
        "outputId": "b356d72b-e5f5-4834-a8f5-a349845b9dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the graph...\n",
            "âœ… LangGraph compiled and ready to run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Final Agent"
      ],
      "metadata": {
        "id": "d8WOWAA_OVzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 7: Run Your Agent!\n",
        "\n",
        "print(\"--- ðŸš€ Test 1: Running Medical RAG Agent (Good Query) ---\")\n",
        "# Define a medical query based on symptoms in your dataset\n",
        "query_good = \"I have high fever, joint pain, and a runny nose. What is my diagnosis?\"\n",
        "\n",
        "inputs_good = {\"question\": query_good, \"documents\": [], \"generation\": \"\", \"question_history\": []}\n",
        "\n",
        "print(f\"\\nUser Query: {query_good}\\n\")\n",
        "# Run app.stream() and print the final 'generate' output\n",
        "for output in app.stream(inputs_good):\n",
        "    for key, value in output.items():\n",
        "        print(f\"\\n--- Output from Node: {key} ---\")\n",
        "        if key == \"generate\":\n",
        "            # Print the final answer clearly\n",
        "            print(\" Â  -> ðŸ¤– **Final Medical Diagnosis:**\\n\" + value['generation'])\n",
        "\n",
        "print(\"\\n--- ðŸ Run Complete ---\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMwycr2HOW6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575853fb-c30d-4ac9-dade-cd8a95fd2888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ðŸš€ Test 1: Running Medical RAG Agent (Good Query) ---\n",
            "\n",
            "User Query: I have high fever, joint pain, and a runny nose. What is my diagnosis?\n",
            "\n",
            "---NODE: RETRIEVE \n",
            "\n",
            "--- Output from Node: retrieve ---\n",
            "---NODE: GRADE DOCUMENTS \n",
            "  -> Grader Decision: Relevant \n",
            "  -> Grader Decision: Relevant \n",
            "  -> Grader Decision: Relevant \n",
            "---CONDITIONAL EDGE \n",
            "  -> Decision: Relevant documents found. GENERATE.\n",
            "\n",
            "--- Output from Node: grade_documents ---\n",
            "---NODE: GENERATE ---\n",
            "\n",
            "--- Output from Node: generate ---\n",
            " Â  -> ðŸ¤– **Final Medical Diagnosis:**\n",
            "```json\n",
            "{\n",
            "    \"Disease\": \"Common Cold\",\n",
            "    \"Key_Symptoms\": [\n",
            "        \"High fever\",\n",
            "        \"Joint pain\",\n",
            "        \"Runny nose\"\n",
            "    ],\n",
            "    \"Description\": \"The common cold is a viral infection of your nose and throat (upper respiratory tract). It's usually harmless, although it might not feel that way. Many types of viruses can cause a common cold. Precautions include drinking vitamin C rich drinks, taking vapour, avoiding cold food, and keeping fever in check.\",\n",
            "    \"Precautions\": [\n",
            "        \"drink vitamin C rich drinks\",\n",
            "        \"take vapour\",\n",
            "        \"avoid cold food\",\n",
            "        \"keep fever in check\"\n",
            "    ],\n",
            "    \"Confidence_Level\": \"Medium\"\n",
            "}\n",
            "```\n",
            "\n",
            "--- ðŸ Run Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRQakhS-KGEo",
        "outputId": "f78e5188-d1a6-47a0-c2c7-e4bc4f7cb99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# ------------------ CACHING FOR STABILITY ------------------\n",
        "\n",
        "@st.cache_resource\n",
        "def create_embedding_model():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": \"cpu\"}\n",
        "    )\n",
        "\n",
        "@st.cache_resource\n",
        "def create_vectorstore(_docs):\n",
        "    embedding_model = create_embedding_model()\n",
        "    retriever = FAISS.from_documents(_docs, embedding=embedding_model)\n",
        "    return retriever.as_retriever(search_kwargs={\"k\":3})\n",
        "\n",
        "@st.cache_resource\n",
        "def create_llm():\n",
        "    model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    # model_id=\"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "\n",
        "    # 4-bit quantization settings for GPU\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# ------------------ LOAD & PROCESS DATA ------------------\n",
        "data_path = '/content/drive/MyDrive/dataset.csv'\n",
        "precaution_path = '/content/drive/MyDrive/symptom_precaution.csv'\n",
        "description_path = '/content/drive/MyDrive/symptom_Description.csv'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    st.error(\"âŒ Data files not found. Please ensure Google Drive is mounted and files are in the correct path.\")\n",
        "    st.stop()\n",
        "\n",
        "try:\n",
        "    df_dataset = pd.read_csv(data_path)\n",
        "    df_precaution = pd.read_csv(precaution_path)\n",
        "    df_description = pd.read_csv(description_path)\n",
        "except Exception as e:\n",
        "    st.error(f\"âŒ Error loading CSV files: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "symptom_cols = [c for c in df_dataset.columns if c.startswith('Symptom_')]\n",
        "df_dataset[symptom_cols] = df_dataset[symptom_cols].fillna('')\n",
        "\n",
        "def combine_symptoms(row):\n",
        "    symptoms = [s.strip().replace('_',' ') for s in row[symptom_cols] if s.strip()]\n",
        "    return ', '.join(list(set(symptoms)))\n",
        "\n",
        "df_dataset['All_Symptoms'] = df_dataset.apply(combine_symptoms, axis=1)\n",
        "df_dataset = df_dataset.drop(columns=symptom_cols).drop_duplicates(subset=['Disease']).reset_index(drop=True)\n",
        "\n",
        "prec_cols = [c for c in df_precaution.columns if c.startswith('Precaution_')]\n",
        "df_precaution[prec_cols] = df_precaution[prec_cols].fillna('No Further Precaution Mentioned')\n",
        "\n",
        "def combine_precautions(row):\n",
        "    precautions = [p.strip() for p in row[prec_cols] if p.strip() != 'No Further Precaution Mentioned']\n",
        "    return ' | '.join(precautions)\n",
        "\n",
        "df_precaution['All_Precaution'] = df_precaution.apply(combine_precautions, axis=1)\n",
        "df_precaution = df_precaution.drop(columns=prec_cols).drop_duplicates(subset=['Disease'])\n",
        "\n",
        "df_final = pd.merge(df_dataset, df_description, on='Disease', how='left')\n",
        "df_final = pd.merge(df_final, df_precaution, on='Disease', how='left')\n",
        "\n",
        "def df_to_docs(df):\n",
        "    docs = []\n",
        "    for _, row in df.iterrows():\n",
        "        content = f\"\"\"\n",
        "        Medical Case Knowledge\n",
        "        Disease: {row['Disease']}\n",
        "        Symptoms: {row['All_Symptoms']}\n",
        "        Description: {row['Description']}\n",
        "        Precaution: {row['All_Precaution']}\n",
        "        \"\"\"\n",
        "        docs.append(Document(page_content=content.strip(), metadata={'source':'medical_dataset','disease':row['Disease']}))\n",
        "    return docs\n",
        "\n",
        "final_rag_docs = df_to_docs(df_final)\n",
        "\n",
        "retriever = create_vectorstore(final_rag_docs)\n",
        "llm = create_llm()\n",
        "\n",
        "# ------------------ STREAMLIT APP ------------------\n",
        "st.set_page_config(page_title=\"Medical RAG Agent\", layout=\"wide\")\n",
        "st.title(\"ðŸ©º Medical RAG Diagnosis Agent\")\n",
        "\n",
        "if 'history' not in st.session_state:\n",
        "    st.session_state.history = []\n",
        "\n",
        "# Sidebar UI\n",
        "with st.sidebar:\n",
        "    st.header(\"Upload Logo\")\n",
        "    uploaded_logo = st.file_uploader(\"Upload your logo (optional)\", type=['png','jpg','jpeg'])\n",
        "    if uploaded_logo:\n",
        "        st.image(uploaded_logo, width=150)\n",
        "\n",
        "    st.header(\"History Controls\")\n",
        "    if st.button(\"Clear History\"):\n",
        "        st.session_state.history = []\n",
        "        st.rerun()\n",
        "\n",
        "user_input = st.text_area(\"Enter your symptoms:\", height=150)\n",
        "\n",
        "if st.button(\"Send\"):\n",
        "    if user_input.strip():\n",
        "        with st.spinner(\"Analyzing symptoms and generating diagnosis...\"):\n",
        "            # 1. Retrieval\n",
        "            docs = retriever.invoke(user_input)\n",
        "            context = '\\n'.join([d.page_content for d in docs])\n",
        "\n",
        "            # 2. Generation Prompt\n",
        "            prompt_template = \"\"\"\n",
        "            You are a highly disciplined medical diagnosis assistant.\n",
        "            Your task is to analyze the 'Context' and the user's 'Symptoms' to provide a diagnosis report.\n",
        "\n",
        "            **STRICT INSTRUCTIONS (MUST FOLLOW):**\n",
        "            1. Output your answer ONLY as a JSON object enclosed in triple backticks (```json ... ```).\n",
        "            2. DO NOT include ANY explanatory text or instructions outside of the JSON block.\n",
        "            3. The JSON object MUST have the EXACT keys: \"Disease\", \"Key_Symptoms\" (as a list of strings), \"Description\" (as a string), \"Precautions\" (as a list of strings), and \"Confidence_Level\" (as a string).\n",
        "            4. Fill ALL fields with the ACTUAL VALUES from the context.\n",
        "            5. Use ONLY the data provided in the 'Context'.\n",
        "            6. If no clear diagnosis is possible, set \"Disease\" to \"Undetermined\".\n",
        "\n",
        "            **CONFIDENCE LEVEL RULES (MUST FOLLOW):**\n",
        "            Determine the \"Confidence_Level\" based ONLY on how many symptoms from the user's input\n",
        "            match the symptoms associated with the disease in the Medical Context:\n",
        "\n",
        "            - If 3 or more symptoms match â†’ \"High\"\n",
        "            - If exactly 2 symptoms match â†’ \"Medium\"\n",
        "            - If exactly 1 symptom matches â†’ \"Low\"\n",
        "            - If 0 symptoms match â†’ \"Very Low\"\n",
        "\n",
        "            Symptoms: {question}\n",
        "            Context: {context}\n",
        "\n",
        "            JSON Report:\n",
        "            \"\"\"\n",
        "\n",
        "            prompt = PromptTemplate.from_template(prompt_template)\n",
        "            rag_chain = prompt | llm | StrOutputParser()\n",
        "            generation = rag_chain.invoke({'question': user_input, 'context': context})\n",
        "\n",
        "        try:\n",
        "            # 3. JSON Parsing and Cleanup\n",
        "\n",
        "            match = re.search(r'```json\\s*(\\{.*?\\}\\s*)```', generation, re.DOTALL)\n",
        "\n",
        "            if match:\n",
        "                json_string = match.group(1).strip()\n",
        "            else:\n",
        "                match = re.search(r'(\\{.*?\\}\\s*)', generation, re.DOTALL)\n",
        "                if match:\n",
        "                    json_string = match.group(1).strip()\n",
        "                else:\n",
        "                    raise ValueError(\"Could not find a valid JSON object structure in the model output.\")\n",
        "\n",
        "            json_string = json_string.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "            # Attempt to load the extracted string\n",
        "            report = json.loads(json_string)\n",
        "            st.success(\"âœ… Diagnosis completed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            report = {'Error':f'Exception parsing JSON: {e}', 'Raw_Output': generation}\n",
        "            st.error(\"âŒ Error during JSON parsing. This is likely due to the LLM's output format.\")\n",
        "\n",
        "        st.session_state.history.append((user_input, report))\n",
        "    else:\n",
        "        st.warning(\"Please enter symptoms before sending.\")\n",
        "\n",
        "# Display Diagnosis History\n",
        "st.header(\"Diagnosis History\")\n",
        "for q, r in st.session_state.history:\n",
        "    with st.chat_message('user'):\n",
        "        st.markdown(q)\n",
        "    with st.chat_message('assistant'):\n",
        "        if 'Error' in r:\n",
        "            st.error(f\"Error: {r['Error']}\")\n",
        "            if 'Raw_Output' in r:\n",
        "                st.subheader(\"Raw Model Output (for debugging):\")\n",
        "                st.code(r['Raw_Output'])\n",
        "        else:\n",
        "            st.success(f\"**Diagnosis:** {r.get('Disease', 'Undetermined')}\")\n",
        "\n",
        "            st.subheader(\"ðŸ“‹ Symptoms Provided\")\n",
        "            key_symptoms = r.get(\"Key_Symptoms\")\n",
        "            if isinstance(key_symptoms, list):\n",
        "                st.markdown(\"- \" + \"  \\n- \".join(key_symptoms))\n",
        "            else:\n",
        "                 st.markdown(f\"**Symptoms:** {key_symptoms}\")\n",
        "\n",
        "\n",
        "            st.subheader(\"ðŸ’¡ Condition Description\")\n",
        "            st.markdown(r.get('Description', 'No description available from the knowledge base.'))\n",
        "\n",
        "            st.subheader(\"âš ï¸ Precautionary Measures\")\n",
        "            precautions = r.get(\"Precautions\")\n",
        "            if isinstance(precautions, list):\n",
        "                st.markdown(\"* \" + \"  \\n* \".join(precautions))\n",
        "            else:\n",
        "                 st.markdown(f\"**Precautions:** {precautions}\")\n",
        "\n",
        "            st.caption(f\"Confidence Level: **{r.get('Confidence_Level', 'N/A')}**\")\n",
        "\n",
        "\n",
        "# Download button\n",
        "if st.session_state.history:\n",
        "    all_reports = [r for _, r in st.session_state.history]\n",
        "    st.download_button(\"Download Reports JSON\", data=json.dumps(all_reports, indent=2), file_name=\"diagnosis_reports.json\")\n",
        "\n",
        "print(\"âœ… app.py file written successfully with robust JSON parsing and display logic.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMKi21sc3xvb",
        "outputId": "8b39aabf-5a9e-496d-cc44-af02accf49e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "print(\"Stopping all active ngrok tunnels...\")\n",
        "ngrok.kill()\n",
        "print(\"All tunnels stopped. Ready for a fresh start.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C63lPHceY5Rr",
        "outputId": "759d34af-6120-4f49-91f7-e22a2e6af66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n",
            "Stopping all active ngrok tunnels...\n",
            "All tunnels stopped. Ready for a fresh start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyngrok\n",
        "!pip install streamlit\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"33SJ0eoJlLHPEV61oqyWpLJraBP_28LFemjgc8m5uFXCKyqtL\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "print(\"ðŸš€ Starting Streamlit app...\")\n",
        "streamlit_proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "time.sleep(15)\n",
        "\n",
        "print(\"Creating Ngrok tunnel...\")\n",
        "public_url = ngrok.connect(addr=\"8501\")\n",
        "\n",
        "print(f\"--- âœ… Deployment Successful! ---\")\n",
        "print(f\"Public URL for your app is: {public_url}\")\n",
        "print(\"Click the link above to access your Medical RAG Agent!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4wnsfKYKPJ-",
        "outputId": "7cd7ed4c-75ef-4d68-cfcb-ef4e8ad6ca85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.3.3)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.5)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n",
            "ðŸš€ Starting Streamlit app...\n",
            "Creating Ngrok tunnel...\n",
            "--- âœ… Deployment Successful! ---\n",
            "Public URL for your app is: NgrokTunnel: \"https://faintish-herbaceously-elva.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "Click the link above to access your Medical RAG Agent!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5yC1Be5TpPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}